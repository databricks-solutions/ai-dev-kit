skill_name: databricks-mlflow-evaluation
description: MLflow 3 GenAI agent evaluation. Use when writing mlflow.genai.evaluate() code, creating @scorer functions, using
  built-in scorers (Guidelines, Correctness, Safety, RetrievalGroundedness), building eval datasets from traces, setting up
  trace ingestion and production monitoring, aligning judges with MemAlign from domain expert feedback, or running optimize_prompts()
  with GEPA for automated prompt improvement.
scorers:
  enabled:
  - pattern_adherence
  - no_hallucinated_apis
  - expected_facts_present
  llm_scorers:
  - Safety
  - guidelines_from_expectations
  default_guidelines:
  - Response must address the user's request completely
  - Code examples must follow documented best practices
  - Response must use modern APIs (not deprecated ones)
  trace_expectations:
    tool_limits:
      Bash: 10
      Read: 20
    token_budget:
      max_total: 100000
    required_tools:
    - Read
    banned_tools: []
    expected_files: []
quality_gates:
  syntax_valid: 1.0
  pattern_adherence: 0.9
  execution_success: 0.8
