# Data Quality Patterns

Patterns for injecting realistic data quality issues into synthetic datasets. Real-world data is messy — these patterns help generate test data that exercises data quality checks, expectations, and cleansing logic.

## Null Injection

### Using dbldatagen percentNulls

The simplest approach — dbldatagen natively supports null injection on any column:

```python
import dbldatagen as dg

spec = (
    dg.DataGenerator(spark, rows=1_000_000, partitions=10)
    .withColumn("id", "long", minValue=1, uniqueValues=1_000_000)
    .withColumn("name", "string", template=r"\\w \\w", percentNulls=0.02)
    .withColumn("email", "string", template=r"\\w.\\w@\\w.com", percentNulls=0.05)
    .withColumn("phone", "string", template=r"(ddd)-ddd-dddd", percentNulls=0.10)
    .withColumn("amount", "decimal(10,2)", minValue=10, maxValue=5000,
                random=True, percentNulls=0.01)
    .build()
)
```

The `percentNulls` parameter accepts a float between 0.0 and 1.0. A value of 0.02 means roughly 2% of rows will have `NULL` for that column.

### Post-Processing Null Injection

When you need to inject nulls into an existing DataFrame (e.g., one not generated by dbldatagen):

```python
from pyspark.sql import functions as F

def inject_nulls(df, column, null_rate=0.02):
    """Replace values with NULL at the given rate."""
    return df.withColumn(
        column,
        F.when(F.rand() < null_rate, F.lit(None)).otherwise(F.col(column))
    )

# Apply to multiple columns with different rates
df = inject_nulls(df, "email", null_rate=0.05)
df = inject_nulls(df, "phone", null_rate=0.10)
df = inject_nulls(df, "address", null_rate=0.03)
```

## Duplicate Injection

Inject exact duplicates by sampling and unioning back:

```python
def inject_duplicates(df, duplicate_rate=0.04):
    """Inject duplicate rows at the given rate (e.g., 0.04 = ~4% duplicates)."""
    duplicates = df.sample(fraction=duplicate_rate)
    return df.unionByName(duplicates)

# 4% of rows will appear twice
dirty_df = inject_duplicates(base_df, duplicate_rate=0.04)
```

For near-duplicates (same ID, slightly different values), modify specific columns in the sampled rows:

```python
def inject_near_duplicates(df, duplicate_rate=0.02):
    """Inject near-duplicates with slight field variations."""
    dupes = df.sample(fraction=duplicate_rate)
    dupes = dupes.withColumn("name", F.concat(F.col("name"), F.lit(" ")))  # trailing space
    dupes = dupes.withColumn("amount", F.col("amount") + F.lit(0.01))     # penny difference
    return df.unionByName(dupes)
```

## Timestamp Variance (Late-Arriving Data)

Simulate late-arriving events by adding random jitter to timestamps:

```python
from pyspark.sql import functions as F

def inject_late_arrivals(df, timestamp_col, max_delay_seconds=60):
    """Add random jitter of +/- max_delay_seconds to a timestamp column."""
    return df.withColumn(
        timestamp_col,
        F.expr(f"""
            {timestamp_col} + make_interval(0,0,0,0,0,0,
                (rand() * {max_delay_seconds * 2} - {max_delay_seconds}))
        """)
    )

# +/- 60 seconds jitter
df = inject_late_arrivals(df, "event_time", max_delay_seconds=60)
```

For more extreme late arrivals (hours or days late), apply to a subset of rows:

```python
# 5% of events arrive 1-24 hours late
df = df.withColumn(
    "event_time",
    F.when(F.rand() < 0.05,
           F.col("event_time") - F.expr("make_interval(0,0,0,0, cast(rand() * 24 as int), 0, 0)"))
    .otherwise(F.col("event_time"))
)
```

## Out-of-Order Events

Shuffle event ordering to simulate out-of-order delivery:

```python
# Complete shuffle — destroys all ordering
unordered_df = df.orderBy(F.rand())

# Partial shuffle — swap adjacent events within windows
from pyspark.sql.window import Window

w = Window.orderBy("event_time")
df = df.withColumn("_rn", F.row_number().over(w))
df = df.withColumn("_swap_rn",
    F.when(F.rand() < 0.1, F.col("_rn") + F.lit(1)).otherwise(F.col("_rn")))
df = df.orderBy("_swap_rn").drop("_rn", "_swap_rn")
```

## Schema Quality Issues

### Mixed Case Values

```python
def inject_case_issues(df, column, rate=0.10):
    """Randomly change casing on string values."""
    return df.withColumn(
        column,
        F.when(F.rand() < rate / 2, F.upper(F.col(column)))
        .when(F.rand() < rate, F.lower(F.col(column)))
        .otherwise(F.col(column))
    )

# 10% of status values will have wrong casing
df = inject_case_issues(df, "status", rate=0.10)
```

### Extra Whitespace

```python
def inject_whitespace(df, column, rate=0.05):
    """Inject leading/trailing whitespace into string values."""
    return df.withColumn(
        column,
        F.when(F.rand() < rate / 2, F.concat(F.lit("  "), F.col(column)))
        .when(F.rand() < rate, F.concat(F.col(column), F.lit("  ")))
        .otherwise(F.col(column))
    )

df = inject_whitespace(df, "name", rate=0.05)
df = inject_whitespace(df, "email", rate=0.03)
```

### Inconsistent Date Formats

Useful for testing ingestion pipelines that must parse multiple date formats:

```python
from pyspark.sql import functions as F

def inject_date_format_issues(df, date_col, rate=0.15):
    """Convert some date values to inconsistent string formats."""
    return df.withColumn(
        date_col,
        F.when(F.rand() < rate * 0.33,
               F.date_format(F.col(date_col), "MM/dd/yyyy"))       # 01/15/2024
        .when(F.rand() < rate * 0.66,
               F.date_format(F.col(date_col), "MMM dd, yyyy"))     # Jan 15, 2024
        .otherwise(
               F.date_format(F.col(date_col), "yyyy-MM-dd"))       # 2024-01-15
    )
```

Note: This converts the column to STRING type. The downstream pipeline is expected to parse and standardize it.

## Configurable Quality Injection

Combine all patterns into a single reusable function:

```python
from pyspark.sql import DataFrame, functions as F

def inject_quality_issues(
    df: DataFrame,
    null_rate: float = 0.02,
    duplicate_rate: float = 0.04,
    late_arrival_seconds: int = 60,
    case_issue_rate: float = 0.05,
    whitespace_rate: float = 0.03,
    timestamp_col: str = None,
    string_cols: list = None,
    nullable_cols: list = None,
) -> DataFrame:
    """Inject configurable data quality issues into a DataFrame.

    Args:
        df: Source DataFrame.
        null_rate: Fraction of nulls to inject into nullable_cols.
        duplicate_rate: Fraction of duplicate rows to add.
        late_arrival_seconds: Max +/- jitter for timestamp_col.
        case_issue_rate: Fraction of string values with case issues.
        whitespace_rate: Fraction of string values with extra whitespace.
        timestamp_col: Column name for timestamp jitter.
        string_cols: Columns to apply case/whitespace issues.
        nullable_cols: Columns to inject nulls into.
    """
    # Null injection
    if nullable_cols:
        for col_name in nullable_cols:
            df = df.withColumn(
                col_name,
                F.when(F.rand() < null_rate, F.lit(None))
                .otherwise(F.col(col_name))
            )

    # Duplicate injection
    if duplicate_rate > 0:
        duplicates = df.sample(fraction=duplicate_rate)
        df = df.unionByName(duplicates)

    # Timestamp jitter
    if timestamp_col and late_arrival_seconds > 0:
        max_s = late_arrival_seconds
        df = df.withColumn(
            timestamp_col,
            F.expr(f"""
                {timestamp_col} + make_interval(0,0,0,0,0,0,
                    (rand() * {max_s * 2} - {max_s}))
            """)
        )

    # String quality issues
    if string_cols:
        for col_name in string_cols:
            df = df.withColumn(
                col_name,
                F.when(F.rand() < case_issue_rate, F.upper(F.col(col_name)))
                .when(F.rand() < whitespace_rate,
                      F.concat(F.lit(" "), F.col(col_name)))
                .otherwise(F.col(col_name))
            )

    return df
```

### Usage

```python
dirty_df = inject_quality_issues(
    df,
    null_rate=0.02,
    duplicate_rate=0.04,
    late_arrival_seconds=120,
    case_issue_rate=0.05,
    whitespace_rate=0.03,
    timestamp_col="event_time",
    string_cols=["name", "email", "status"],
    nullable_cols=["email", "phone", "address"],
)
```
